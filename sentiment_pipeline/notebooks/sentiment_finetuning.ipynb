{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3 Sentiment Fine-Tuning\n",
        "\n",
        "This Colab-friendly notebook fine-tunes **Mistral-7B-Instruct-v0.3** on labeled Yahoo Finance headlines using LoRA adapters.\n",
        "\n",
        "> **Prereqs**\n",
        "> 1. Upload the project folder to Google Drive (or clone via git).\n",
        "> 2. Generate labeled data via `data_collection.py` → `label_news.py` → `prepare_dataset.py`.\n",
        "> 3. Ensure you have GPU runtime (T4/A100 preferred).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Mount Google Drive when running in Colab\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Google Colab not detected; skipping drive mount.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "PROJECT_ROOT = os.environ.get(\"PROJECT_ROOT\", \"/content/drive/MyDrive/Personalized-Investment-Recommendation-System\")\n",
        "if not os.path.exists(PROJECT_ROOT):\n",
        "    PROJECT_ROOT = os.environ.get(\"PROJECT_ROOT\", os.getcwd())\n",
        "\n",
        "print(f\"Using project root: {PROJECT_ROOT}\")\n",
        "\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "REQ_PATH = os.path.join(PROJECT_ROOT, \"sentiment_pipeline\", \"requirements.txt\")\n",
        "if os.path.exists(REQ_PATH):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", REQ_PATH])\n",
        "else:\n",
        "    print(\"requirements.txt not found; ensure deps are installed locally.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from datasets import load_from_disk\n",
        "\n",
        "from sentiment_pipeline.utils.config_loader import load_config\n",
        "\n",
        "load_dotenv()\n",
        "config = load_config(os.path.join(PROJECT_ROOT, \"sentiment_pipeline\", \"config.yaml\"))\n",
        "\n",
        "dataset_root = os.path.normpath(os.path.join(PROJECT_ROOT, config[\"paths\"][\"hf_dataset_dir\"]))\n",
        "train_ds = load_from_disk(os.path.join(dataset_root, \"train\"))\n",
        "val_ds = load_from_disk(os.path.join(dataset_root, \"validation\"))\n",
        "print(train_ds[:2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "if config[\"wandb\"].get(\"project\"):\n",
        "    wandb.login()\n",
        "    wandb.init(project=config[\"wandb\"][\"project\"], entity=config[\"wandb\"].get(\"entity\"))\n",
        "else:\n",
        "    print(\"wandb project not configured; skipping tracking.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "training_cfg = config[\"training\"]\n",
        "model_name = training_cfg[\"model_name\"]\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "tokenizer.padding_side = \"right\"\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=training_cfg[\"lora_r\"],\n",
        "    lora_alpha=training_cfg[\"lora_alpha\"],\n",
        "    lora_dropout=training_cfg[\"lora_dropout\"],\n",
        "    target_modules=training_cfg[\"target_modules\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=os.path.join(PROJECT_ROOT, \"sentiment_pipeline\", \"models\", \"sentiment_model\"),\n",
        "    per_device_train_batch_size=training_cfg[\"batch_size\"],\n",
        "    per_device_eval_batch_size=training_cfg[\"batch_size\"],\n",
        "    gradient_accumulation_steps=training_cfg[\"gradient_accumulation_steps\"],\n",
        "    learning_rate=training_cfg[\"learning_rate\"],\n",
        "    num_train_epochs=training_cfg[\"num_epochs\"],\n",
        "    warmup_steps=training_cfg[\"warmup_steps\"],\n",
        "    fp16=training_cfg[\"fp16\"],\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(training_args.output_dir)\n",
        "tokenizer.save_pretrained(training_args.output_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_prompt = \"Headline: Apple announces record-breaking services revenue\\nSentiment:\"\n",
        "output = trainer.model.generate(\n",
        "    **tokenizer(sample_prompt, return_tensors=\"pt\").to(trainer.model.device),\n",
        "    max_new_tokens=config[\"inference\"][\"max_new_tokens\"],\n",
        ")\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
